#!/usr/bin/env python3
"""
æœ¬åœ°è§’è‰²è¯†åˆ«æµ‹è¯•è„šæœ¬
æ— éœ€ Dockerï¼Œç›´æ¥ä½¿ç”¨ Python ç¯å¢ƒè¿›è¡Œè§’è‰²è¯†åˆ«
"""

import sys
import os
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
char_recognition_path = project_root / "apps" / "character-recognition"

if char_recognition_path.exists():
    sys.path.insert(0, str(char_recognition_path))
    print(f"âœ“ æ·»åŠ è·¯å¾„: {char_recognition_path}")
else:
    print(f"âœ— è·¯å¾„ä¸å­˜åœ¨: {char_recognition_path}")
    sys.exit(1)

def install_required_packages():
    """å®‰è£…å¿…è¦çš„åŒ…"""
    required_packages = [
        "hanlp>=2.1.0",
        "sentence-transformers",
        "loguru",
        "numpy"
    ]

    for package in required_packages:
        try:
            __import__(package.replace("-", "_").split(">=")[0].split("==")[0])
            print(f"âœ“ {package} å·²å®‰è£…")
        except ImportError:
            print(f"âš  éœ€è¦å®‰è£…: {package}")
            print(f"è¿è¡Œ: pip install {package}")

def simple_character_recognition(text):
    """ç®€å•çš„è§’è‰²è¯†åˆ«å®ç°ï¼ˆåŸºäºå¯å‘å¼è§„åˆ™ï¼‰"""
    import re

    # å¸¸è§ä¸­æ–‡äººåæ¨¡å¼
    name_patterns = [
        r'([A-Za-z\u4e00-\u9fff]{2,4})(?:è¯´|é“|ç¬‘|çœ‹|æƒ³|å«|å–Š)',
        r'"([^"]+)"(?:è¯´|é“|é—®|ç­”)',
        r'"([^"]+)"\s*(?:è¯´|é“|é—®|ç­”)',
        r'([A-Za-z\u4e00-\u9fff]{2,4})(?:ï¼Œ|ã€‚|ï¼š|ï¼|ï¼Ÿ)',
    ]

    characters = {}

    for pattern in name_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            name = match.strip()
            if len(name) >= 2 and len(name) <= 4:
                # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯äººåçš„è¯
                if not any(word in name for word in ['è¿™', 'é‚£', 'ä»–', 'å¥¹', 'æˆ‘', 'ä½ ', 'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'ä¸ª']):
                    characters[name] = characters.get(name, 0) + 1

    # æŒ‰å‡ºç°æ¬¡æ•°æ’åº
    sorted_chars = sorted(characters.items(), key=lambda x: x[1], reverse=True)

    return [
        {
            "name": name,
            "aliases": [],
            "appearance_count": count,
            "importance": "é«˜" if count > 10 else "ä¸­" if count > 3 else "ä½"
        }
        for name, count in sorted_chars if count >= 2
    ]

def advanced_character_recognition(text):
    """å°è¯•ä½¿ç”¨ HanLP è¿›è¡Œè§’è‰²è¯†åˆ«"""
    try:
        import hanlp

        print("âœ“ ä½¿ç”¨ HanLP è¿›è¡Œé«˜çº§è§’è‰²è¯†åˆ«")

        # åŠ è½½ HanLP NER æ¨¡å‹
        try:
            HanLP = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)
        except:
            print("âš  æ— æ³•åŠ è½½ HanLP NER æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€ç‰ˆæœ¬")
            HanLP = hanlp.load(hanlp.pretrained.ner.MSRA_NER_ALBERT_BASE_ZH)

        # åˆ†æ‰¹å¤„ç†é•¿æ–‡æœ¬
        max_length = 512
        text_chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]

        all_entities = []
        for chunk in text_chunks:
            try:
                result = HanLP(chunk)
                if isinstance(result, dict) and 'ner' in result:
                    entities = result['ner']
                    all_entities.extend(entities)
                elif isinstance(result, list):
                    all_entities.extend(result)
            except Exception as e:
                print(f"âš  å¤„ç†æ–‡æœ¬å—æ—¶å‡ºé”™: {e}")
                continue

        # ç»Ÿè®¡äººå
        character_counts = {}
        for entity in all_entities:
            if isinstance(entity, (list, tuple)) and len(entity) >= 3:
                name, entity_type, confidence = entity[:3]
                if entity_type == 'PER' and confidence > 0.5:
                    character_counts[name] = character_counts.get(name, 0) + 1

        # ç”Ÿæˆç»“æœ
        results = []
        for name, count in sorted(character_counts.items(), key=lambda x: x[1], reverse=True):
            if count >= 2:  # è‡³å°‘å‡ºç° 2 æ¬¡
                results.append({
                    "name": name,
                    "aliases": [],
                    "appearance_count": count,
                    "importance": "é«˜" if count > 10 else "ä¸­" if count > 3 else "ä½"
                })

        return results

    except ImportError:
        print("âš  HanLP æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•è¯†åˆ«")
        return None
    except Exception as e:
        print(f"âš  HanLP è¯†åˆ«å¤±è´¥: {e}")
        return None

def load_target_characters():
    """åŠ è½½ç›®æ ‡è§’è‰²æ•°æ®"""
    try:
        with open('characters.txt', 'r', encoding='utf-8') as f:
            content = f.read()
            # ç§»é™¤ JSON æ ¼å¼çš„å¤–å±‚ï¼Œåªæå–è§’è‰²ä¿¡æ¯
            if content.strip().startswith('['):
                characters = json.loads(content)
                return {char['name']: char for char in characters}
    except Exception as e:
        print(f"âš  æ— æ³•åŠ è½½ characters.txt: {e}")
    return {}

def compare_results(detected, target):
    """æ¯”è¾ƒè¯†åˆ«ç»“æœä¸ç›®æ ‡ç»“æœ"""
    print("\n" + "="*60)
    print("ğŸ“Š è¯†åˆ«ç»“æœå¯¹æ¯”åˆ†æ")
    print("="*60)

    detected_names = {char['name'] for char in detected}
    target_names = {char['name'] for char in target.values()}

    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ£€æµ‹åˆ°è§’è‰²æ•°: {len(detected)}")
    print(f"  ç›®æ ‡è§’è‰²æ•°: {len(target)}")
    print(f"  é‡å è§’è‰²æ•°: {len(detected_names & target_names)}")
    print(f"  æ£€æµ‹å‡†ç¡®ç‡: {len(detected_names & target_names) / max(len(target_names), 1) * 100:.1f}%")

    print(f"\nâœ… æ­£ç¡®è¯†åˆ«çš„è§’è‰²:")
    for name in sorted(detected_names & target_names):
        detected_char = next(c for c in detected if c['name'] == name)
        target_char = target[name]
        print(f"  {name}: æ£€æµ‹æ¬¡æ•°={detected_char['appearance_count']}, ç›®æ ‡æ¬¡æ•°={target_char['appearance_count']}")

    print(f"\nâŒ é—æ¼çš„è§’è‰²:")
    for name in sorted(target_names - detected_names):
        target_char = target[name]
        print(f"  {name}: ç›®æ ‡æ¬¡æ•°={target_char['appearance_count']}")

    print(f"\nğŸ” è¯¯è¯†åˆ«çš„è§’è‰²:")
    for name in sorted(detected_names - target_names):
        detected_char = next(c for c in detected if c['name'] == name)
        print(f"  {name}: æ£€æµ‹æ¬¡æ•°={detected_char['appearance_count']}")

def main():
    print("ğŸš€ æœ¬åœ°è§’è‰²è¯†åˆ«æµ‹è¯•")
    print("="*60)

    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    if not Path('1.txt').exists():
        print("âŒ æµ‹è¯•æ–‡ä»¶ 1.txt ä¸å­˜åœ¨")
        return

    # åŠ è½½æµ‹è¯•æ–‡æœ¬
    print(f"\nğŸ“– è¯»å–æµ‹è¯•æ–‡ä»¶...")
    try:
        with open('1.txt', 'r', encoding='utf-8') as f:
            text = f.read()
        print(f"âœ“ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return

    # å®‰è£…æ£€æŸ¥
    print(f"\nğŸ”§ æ£€æŸ¥ä¾èµ–...")
    install_required_packages()

    # åŠ è½½ç›®æ ‡ç»“æœ
    print(f"\nğŸ“‹ åŠ è½½ç›®æ ‡ç»“æœ...")
    target_chars = load_target_characters()
    if target_chars:
        print(f"âœ“ åŠ è½½äº† {len(target_chars)} ä¸ªç›®æ ‡è§’è‰²")
    else:
        print("âš  æ— æ³•åŠ è½½ç›®æ ‡ç»“æœï¼Œå°†åªæ˜¾ç¤ºè¯†åˆ«ç»“æœ")

    # å°è¯•é«˜çº§è¯†åˆ«
    print(f"\nğŸ¤– å¼€å§‹è§’è‰²è¯†åˆ«...")
    results = advanced_character_recognition(text)

    # å¦‚æœé«˜çº§è¯†åˆ«å¤±è´¥ï¼Œä½¿ç”¨ç®€å•è¯†åˆ«
    if not results:
        print("ğŸ”„ ä½¿ç”¨ç®€å•è§„åˆ™è¯†åˆ«...")
        results = simple_character_recognition(text)

    if not results:
        print("âŒ è¯†åˆ«å¤±è´¥")
        return

    print(f"\nâœ… è¯†åˆ«å®Œæˆï¼Œå…±è¯†åˆ«åˆ° {len(results)} ä¸ªè§’è‰²:")
    for i, char in enumerate(results[:10], 1):  # åªæ˜¾ç¤ºå‰ 10 ä¸ª
        print(f"  {i:2d}. {char['name']:10s} ({char['appearance_count']:3d}æ¬¡) - {char['importance']}")

    # ä¿å­˜ç»“æœ
    output_file = 'recognition_result.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # å¦‚æœæœ‰ç›®æ ‡ç»“æœï¼Œè¿›è¡Œå¯¹æ¯”
    if target_chars:
        compare_results(results, target_chars)

if __name__ == "__main__":
    main()